{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae3701c-d259-4f2d-8faf-a54065d9b29e",
   "metadata": {},
   "source": [
    "### 1 . what are missing values in a dataset ? why is it essential to handle values ? name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d13e14e-d06a-4254-9a6f-91147a8dbe59",
   "metadata": {},
   "source": [
    "##### Missing values :\n",
    "- Missing vlaues in dataset refer to the absence of data for one or more variables in certain observations or instance.they can occur for various reasons, such as data entry error,equipment malfuntion or non-response from survey participants.\n",
    "\n",
    "##### Essential to Handle values :\n",
    "- It is essential because they can adversely affect the performance and reliability of statistical analyses and machine learning models.ignoring missing values or not handling them properly can lead to biased results,reduced statistical power, inaccurate predictions.\n",
    "\n",
    "##### Algorithms that are not effected by missing values are :\n",
    "1. Decision Tree\n",
    "2. Random Forests\n",
    "3. Gradient Boosting\n",
    "4. Naive Bayes\n",
    "5. K-nearest Neighbours(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb284b-8cf2-4c0c-a720-134b1c74094d",
   "metadata": {},
   "source": [
    "### 2 . List down tehcniques used to handle missing data.give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790b6ee-124f-44ee-94f8-8e3f7701b7b0",
   "metadata": {},
   "source": [
    "##### Techniques Used to handle missing data\n",
    "1. Mean / Median / Mode\n",
    "2. Interpolate\n",
    "3. forward or backward fill\n",
    "4. Delete rows and cloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e5bb1d-211e-4b9e-8318-f32fd12d5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    4.0\n",
      "4    5.0\n",
      "Name: A, dtype: float64 0     6.0\n",
      "1     7.0\n",
      "2     8.0\n",
      "3     9.0\n",
      "4    10.0\n",
      "Name: B, dtype: float64 0    1.0\n",
      "1    2.0\n",
      "2    4.0\n",
      "3    4.0\n",
      "4    5.0\n",
      "Name: A, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Mean/Median/mode technique \n",
    "import pandas as pd\n",
    "\n",
    "## sample dataframe\n",
    "data = {'A':[1,2,None,4,5],\n",
    "       'B': [6,7,None,9,10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "## mean imputation\n",
    "mean = df['A'].fillna(df['A'].mean())\n",
    "\n",
    "## median imputation\n",
    "median = df['B'].fillna(df['B'].median())\n",
    "\n",
    "## Mode imputation\n",
    "mode = df['A'].fillna(df['A'].mode())\n",
    "\n",
    "## printing the results\n",
    "print(mean,median,mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b9da43-cd51-44b3-8951-2e485b8efa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A\n",
      "0  1.0\n",
      "1  2.0\n",
      "2  3.0\n",
      "3  3.5\n",
      "4  4.0\n",
      "5  5.0\n"
     ]
    }
   ],
   "source": [
    "## Inrepolate\n",
    "import pandas as pd\n",
    "\n",
    "## sample dataframe\n",
    "data = {'A' : [1,None,3,None,4,5]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "## interpolate\n",
    "df['A'] = df['A'].interpolate(method='linear')\n",
    "\n",
    "## printing results\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cce4371-4302-428c-96fe-067742ec899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  1.0   8.0\n",
      "2  3.0   8.0\n",
      "3  3.0  10.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "## Forward and backward fill\n",
    "import pandas as pd\n",
    "\n",
    "## sample dataFrame\n",
    "data = {'A': [1, None, 3, None, 5],\n",
    "       'B': [6, None, 8, None, 10]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "## Forward fill\n",
    "df['A'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "## Backward fill\n",
    "df['B'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6882e38f-52e2-4f5f-8775-41715b226014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deleting rows:\n",
      "      A    B   C\n",
      "2  3.0  7.0  13\n",
      "4  5.0  9.0  15 \n",
      "After deleting columns : \n",
      "     C\n",
      "0  11\n",
      "1  12\n",
      "2  13\n",
      "3  14\n",
      "4  15\n"
     ]
    }
   ],
   "source": [
    "## delete rows and columns\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'A': [1, None, 3, None, 5],\n",
    "        'B': [None, 6, 7, None, 9],\n",
    "       'C':[11,12,13,14,15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop columns with any missing value\n",
    "col_new = df.dropna(axis=1)\n",
    "\n",
    "# Drop rows with any missing value\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"After deleting rows:\\n\",df,\"\\nAfter deleting columns : \\n\",col_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17b1ef-ae65-48c7-803f-9130c6d0f82e",
   "metadata": {},
   "source": [
    "### 3 . Explain the imbalanced data.What will happen if imabalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727d6de-d570-4f09-b92e-0e15417213c8",
   "metadata": {},
   "source": [
    "##### Imbalanced data :\n",
    "- it refers to a situation in a classification problem where the classes are not represented equally.in other words,one class has significantly fewer instances than the other class. imbalanced data is common issue in many real world scenarios.\n",
    "\n",
    "##### If imbalanced data is not handled :\n",
    "- several problems can arise such as \n",
    "1. Biased models : models may exhibit bias towards majority calss.\n",
    "2. Poor Generalization : It can lead to poor generalization  of the model to unseen data.\n",
    "3. Misleading Evaluation metrics : Traditoinal Evaluation metrics like accuracy can be misleading .\n",
    "4. Loss of important information : ignoring minority class can result in loss of valuable information\n",
    "5. Increased False Negatives : in scenarios where correctly identifying minority class is crucial, imbalanced data can lead to an increase in false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ed89f-798e-4ae2-b032-2dc6f1406374",
   "metadata": {},
   "source": [
    "### 4 . What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47acfd5f-dbb5-47f9-b5f0-338c3d76277f",
   "metadata": {},
   "source": [
    "##### Up-sampling and Down-Sampling :\n",
    "- These two are common techniques used to handle imbalanced data by either increasing the representation of minority class(upsampling) or reducing the representation of majority class(down sampling)\n",
    "\n",
    "##### Example for Up-sampling :\n",
    "- Lets consider a credit card fraud detection dataset where the majority of transactions are legitimate while only small percentage are farudulent. in this case the fraudulent transactions represent the minority class. since accurately identifying fraudulent transactions is crucial, we might up-sample the minority class to ensure that the model is trained on a more balanced dataset, improving its ability to detect fruad.\n",
    "\n",
    "##### Example for Down-sampling :\n",
    "- Consider a dataset for disease diagnosis where the positive cases represent minority class,while the negative cases represent the maajority class.if the majority class heavily outweighs minority class,the training model on this imbalanced dataset may lead to biased results. inthis scenario down-sampling the majority to balance the dataset can help prevent the model from being biased towards predicting negative cases, thus improving its ability to correctly classify positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609fc475-c652-43e5-bb7f-dc4edafed580",
   "metadata": {},
   "source": [
    "### 5 . What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1d710-1472-4af0-9544-bab351127468",
   "metadata": {},
   "source": [
    "##### Data Augmentation :\n",
    "- it is a technique used to artificially increase the size and diversity of a dataset by applying various tranformations to exsiting data samples. this technique is commonly used in machine learning and deep learning tasks. particularly in scenarios where original dataset is limited or imbalanced. by augmenting the dataset with transformed versions of existing samples, the model ca learn from more diverse range of data.potentially improving its robustness and generalization performance.\n",
    "\n",
    "##### SMOTE :\n",
    "- Synthetic Minority Over-sampling Technique (SMOTE) is specifically designed to address datasets by generating synthetic samples for the minority class. it works by synthesizing new instances of the minority class using existing instances rather than simply duplicating them.SMOTE algorithmically creates synthetic samples by interpolating between existing minority class instances.\n",
    "\n",
    "##### How SMOTE works :\n",
    "1. Selecting a sample\n",
    "2. Finding nearest neighbors\n",
    "3. Generating synthetic samples\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65bb0ee-a590-4ee7-b40e-a7dc03cd3602",
   "metadata": {},
   "source": [
    "### 6 . What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85622317-c799-4a93-9ae0-5e66eac95862",
   "metadata": {},
   "source": [
    "##### Outliers :\n",
    "- Outliers are data points that significantly deviate from the rest of the observations in a dataset. they can be usually high or low values compared to the majority of the data and may represent genuine extreme observations or error in data collection or recording.\n",
    "\n",
    "##### why is it Essential to handle outliers :\n",
    "1. preserving model perormance : Outliers can gave a disproportionate impact on statistical analyses and machine learning models,leading to biased estimates and poor model performance.\n",
    "2. Improving interpretability : outliers can skew summary statistics and visualizations, making it difficult to interpret the underlying data patterns accurately.\n",
    "3. Enhancing data Quality : Outliers can sometimes be indicative of error or anomalies in the data. addressing outliers helps improve the overall quality and integrity of the dataset.\n",
    "4. Ensuring fair comparisons : outliers can disort comparisons between groups or across different time periods. we can ensure fair and meaningful data driven decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12007f0b-80e4-4aa7-b5f8-7490a0a3ccdb",
   "metadata": {},
   "source": [
    "### 7 . You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cba23-914b-4d8b-bae2-3a1f0f96cd4a",
   "metadata": {},
   "source": [
    "- When dealing with missing data in customer data analysis. several techniques can be used to gandle msising values appropriately.\n",
    "\n",
    "##### Following are the techniques available :\n",
    "- Deletion : remove observations with missing values from dataset.\n",
    "1. list wise deletion\n",
    "2. pairwise deletion\n",
    "\n",
    "- Imputation : Fill in missing values with estimated or calculated values\n",
    "1. Mean/Median/Mode imputation\n",
    "2. Regression imputation\n",
    "3. K-nearest neighbours (KNN)\n",
    "4. Multiple Imputation\n",
    "\n",
    "- Prediction models : Build predictive models to estimate missing values based on other varibales in the dataset. \n",
    "\n",
    "- Flagging : add an indicator variable to denote whether a value is missing.this allows models to account for missingess of data as a seperate category.\n",
    "\n",
    "- Use of spicialized libraries : taking advantage of pandas,scikit-learn,fancyimpute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd30944-d56f-4a5a-a6e3-fd7b91151a19",
   "metadata": {},
   "source": [
    "### 8 . You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90314f38-7fbf-42c7-b2f3-f64d57232c55",
   "metadata": {},
   "source": [
    "- Determining whether missing data is missing completely at random (MCAR),missing at random (MAR),Missing not at random(MNAR) is essential for understanding the nature of missingness and choosing appropriate strategies to handle it. \n",
    "\n",
    "##### Stategies to use :\n",
    "1. Visual inspection :\n",
    "create visualizations such as heatmaps or missingness matrices to visually inspect the patterns of missing data across variables.\n",
    "\n",
    "2. Statistical test : Perform statistical tests to asses the randomness of missingness. \n",
    "- Little's MCAR test \n",
    "- Chi-square Test\n",
    "\n",
    "\n",
    "3. Analysis of missing data mechanism : Consider the mechanisms underlying missing data and how they relate to observed variables.\n",
    "- MAR assumption\n",
    "- MNAR assumption\n",
    "\n",
    "4. Pattern Recognition : use clustering or classification algorithms to identify patterns in missing data.\n",
    "\n",
    "5. subgroup analysis : conduct subgroup analysis to investigate whether missingness varies across different subgroups of the data. \n",
    "\n",
    "6. Consulting subject matter experts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fc5e5-8db9-4233-96c8-36760e7ec052",
   "metadata": {},
   "source": [
    "### 9 . Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457831b-15d6-47c3-afeb-053932457a45",
   "metadata": {},
   "source": [
    "- When dealing with imbalanced datasets in medical diagnosis project. where the majority of patients do not have the condition of interest. while only a small percentage do, its essential to use evaluation metrics and strategies that appropriately assess the performance of the machine learning model. \n",
    "\n",
    "##### Strategies we can use to asses performance :\n",
    "\n",
    "1. Use appropriate evaluation metrics :\n",
    "\n",
    "- Precision,Recall, and F1-score : these metrics are commonly used for imbalanced dataset,especially in medical diagnosis tasks.\n",
    "- Area under the receiver operating characteristic curve (AUC-ROC) : it measures the model's ability to discriminate between positive and negative instances across different thresholds. it is insensitive to imbalances.\n",
    "- Area under the Precision-Recall curve (AUC-PR): it summerizes the precsion-recall trade-off across different threshold settings.\n",
    "\n",
    "2. Stratified cross-validation : use techniques like k-fold validation to ensure that each fold of the dataset maintains the same class distribution as the original dataset. this helps prevent biased performance estimates due to class imbalance.\n",
    "\n",
    "3. Class weights : Ajust the class weights in the machine learning algorithm to penalize misclassifications of the minority class more heavily. scikit-learn allows you to assign higher weights to minority class instances during training to account for class imbalance.\n",
    "\n",
    "4. resampling techniques : Use techniques as Over-sampling the minority class or undersampling the majority class to balance the class distribution before training the model.\n",
    "\n",
    "5. Ensemble methods : Utilize the ensemble methods like bagging or boosting with base leanrners trained on balanced subsets of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eab16f-a5fa-4d70-a84d-c35f6c9496f3",
   "metadata": {},
   "source": [
    "### 10 . When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d8bd52-9653-49e7-b8b0-50ac67434340",
   "metadata": {},
   "source": [
    "### Methods to use down-sampling majority class :\n",
    "\n",
    "1. Random Under-sampling : Randomly remove samples from the majority class until the class distribution is balanced. this method may lead to loss of information.\n",
    "\n",
    "2. Cluster Centroid : Use clustering techniques to identify centrods of clusters formed by the majority class. then randomly remove the samples from each clusters until the distribution is balanced.\n",
    "\n",
    "3. Tomek links : identify tomek links which are pairs of samples from different classes that are nearest neighbours of each other. remove samples from majority class to in tomek links to create a more balanced dataset.\n",
    "\n",
    "4. Edited nearest neighbours (ENN) : Remove samples from majority class whose class label differs from the class label of at leat half of their k nearest neighbours. this methods removes samples from majority class.\n",
    "\n",
    "5. instance Hardness threshold(IHT) : Use a hardness measure to identify instances that are easy to classify and remove them form the majority class.\n",
    "\n",
    "6. Nearmiss : select samples from the majority class based on thier distnace to the minority class samples. nearmiss algorithm select samples that are closes to the minority samples.\n",
    "\n",
    "7. Down-sampling With replacement : Randomly select smaples form the majority class and remove them until the desired class distribution is achieved. this method allows duplicate samples to be included in the down-sampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb8ba7-1b01-4262-8bef-d4130ae4638e",
   "metadata": {},
   "source": [
    "### 11 . You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c073a-ec30-4f72-be5b-840f22a2ecc7",
   "metadata": {},
   "source": [
    "##### Methods we can use to up-sample the minority class :\n",
    "1. Random Over-sampling : randomly diplicate samples from the minority class until the class distribution is balanced. this method is simple to implement but may lead to overfitting if the duplicated samples introduce noise.\n",
    "\n",
    "2. SMOTE : Generate synthetic samples for minority class by interpolating between existing minority class samples. it creates new instnaces by selecting pairs of nearest neighnours and adding a random freaction of the difference betweeen the two of the feature values of minority class instance \n",
    "\n",
    "3. ASASYNC(Adaptive Synthetic Sampling) ; similar to SMOTE it generates synthetic samples for the minority, however it adapts the sampling distribution based on the local density of minority class.focusing more on regions of the feature space with fewer samples.\n",
    "\n",
    "4. SMOTE-ENN : combine SMOTE with with Edited Nearest Neighbour(ENN) under sampling. first generate synthetic samples using SMOTE then apply ENN to remove noisy samples from both majority and minority class.\n",
    "\n",
    "5. SMOTE-Tomek : combine SMOTE with tomek links under-sampling. generate synthetic samples using SMOTE, then remove tomek links. this improves separation between classes.\n",
    "\n",
    "6. Cluster-based Over-sampling : Use clustering techniques to identify clsuters of minority class samples and then generate synthetic samples within each clsuter. this approach ensures that synthetic samples are generated in regions of the feature space where minority class samples are densely located.\n",
    "\n",
    "7. ADASYNC with different costs : use ADASYNC with different costs for misclassification to focus on generating synthetic samples for regions of the  feature space where the cost of misclassyfying minority class instances is higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
