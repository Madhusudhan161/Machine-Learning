{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0ff47a-19aa-4937-b480-e3b71fa3108a",
   "metadata": {},
   "source": [
    "### 1 . What is min-max scaling, how is it used in data preprocessing ? provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882949b-2479-4bed-b04c-863b6c2ddd52",
   "metadata": {},
   "source": [
    "##### Min-Max scaling : \n",
    "- it is a part of feature extraction. and its available in Scikit learn module along with other preprocessing packages. min-max scaling transforms data by scaling features to a given range. by default these range are [0,1].\n",
    "\n",
    "##### How is it used in data preprocessing :\n",
    "- we can use min-max scaling to normalize data into range of (0,1) or any desired range. with help of this package we can first FIT the data into mean and standard deviation and later on Transform the data using TRANSFORM by calling the object and executing the data into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697278cc-1399-4c56-b661-8b33074f9978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04359673, 0.04026846, 0.06475904],\n",
       "       [0.02152589, 0.02684564, 0.        ],\n",
       "       [0.0373297 , 0.04362416, 0.07108434],\n",
       "       ...,\n",
       "       [0.11280654, 0.10067114, 0.        ],\n",
       "       [0.03051771, 0.03355705, 0.        ],\n",
       "       [0.10490463, 0.09395973, 0.10120482]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example for MIn-Max scaling\n",
    "## importing modules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "\n",
    "## importing dataset\n",
    "df=sns.load_dataset('taxis')\n",
    "\n",
    "## using minmax scaler \n",
    "## calling function using object \n",
    "obj = MinMaxScaler()\n",
    "\n",
    "## aplying the minmax to data\n",
    "obj.fit_transform(df[['distance','fare','tip']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6506549-8be6-438e-97d8-e712edd5e73a",
   "metadata": {},
   "source": [
    "### 2 . What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bedc55-c699-4c76-b199-ab4bb6f70129",
   "metadata": {},
   "source": [
    "##### Unit vector :\n",
    "- Unit vector scaling is done keeping in mind the whole feature vector in a unit lenght. this normally requires dividing each component by the euclidean length of the vector. in certain applications, it can be practical to use L1 norm of the featue vector. \n",
    "\n",
    "##### Difference between Unit vector and Min-max scaling\n",
    "- unit vector only effects the magnitude of the feature vectors, keeping thier direction intact. Min-max scales the feature values to fit within a specified range, altering both the magnitude and the distribution of feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97b88df6-36d6-4664-b66e-437dd5e1ef8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.213461</td>\n",
       "      <td>0.933894</td>\n",
       "      <td>0.286839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156064</td>\n",
       "      <td>0.987747</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171657</td>\n",
       "      <td>0.939731</td>\n",
       "      <td>0.295702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.267899</td>\n",
       "      <td>0.939386</td>\n",
       "      <td>0.213971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.231742</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>0.118017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>0.160133</td>\n",
       "      <td>0.960800</td>\n",
       "      <td>0.226322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.951563</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.968117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>0.183497</td>\n",
       "      <td>0.983020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>0.242956</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>0.212034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6433 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Distance      Fare       Tip\n",
       "0     0.213461  0.933894  0.286839\n",
       "1     0.156064  0.987747  0.000000\n",
       "2     0.171657  0.939731  0.295702\n",
       "3     0.267899  0.939386  0.213971\n",
       "4     0.231742  0.965592  0.118017\n",
       "...        ...       ...       ...\n",
       "6428  0.160133  0.960800  0.226322\n",
       "6429  0.307453  0.951563  0.000000\n",
       "6430  0.250500  0.968117  0.000000\n",
       "6431  0.183497  0.983020  0.000000\n",
       "6432  0.242956  0.946580  0.212034\n",
       "\n",
       "[6433 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example for Unit vector \n",
    "## importing unit vector from sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "\n",
    "## using normalize on data\n",
    "pd.DataFrame(normalize(df[['distance','fare','tip']]),columns=['Distance','Fare','Tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e490d-423e-4c71-9832-2b1c5580cd2a",
   "metadata": {},
   "source": [
    "### 3 . What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04461c2c-0124-4084-9d14-366554cf2800",
   "metadata": {},
   "source": [
    "##### PCA (principle component analysis ):\n",
    "- It is a dimensionality reduction and machine learning method used to simplify a large data set into a smaller set while still maintaining significant patterns and trends.\n",
    "\n",
    "##### How is it used in dimensionality reduction :\n",
    "- it allows you to select subset of them that capture most of the variance in the data while reducing its dimensionality. \n",
    "- by selecting sufficient principal components you can reduce the dimensionality of the data while retaining most of its essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75cec5ac-6096-4482-ac9e-b778be0e2f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.264703</td>\n",
       "      <td>0.480027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.080961</td>\n",
       "      <td>-0.674134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.364229</td>\n",
       "      <td>-0.341908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.299384</td>\n",
       "      <td>-0.597395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.389842</td>\n",
       "      <td>0.646835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.870503</td>\n",
       "      <td>0.386966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.564580</td>\n",
       "      <td>-0.896687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.521170</td>\n",
       "      <td>0.269069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.372788</td>\n",
       "      <td>1.011254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.960656</td>\n",
       "      <td>-0.024332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "0   -2.264703  0.480027\n",
       "1   -2.080961 -0.674134\n",
       "2   -2.364229 -0.341908\n",
       "3   -2.299384 -0.597395\n",
       "4   -2.389842  0.646835\n",
       "..        ...       ...\n",
       "145  1.870503  0.386966\n",
       "146  1.564580 -0.896687\n",
       "147  1.521170  0.269069\n",
       "148  1.372788  1.011254\n",
       "149  0.960656 -0.024332\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of PCA\n",
    "## importing modules\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## importing data\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "## standatdizing the features\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "## PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "pd.DataFrame(pca.fit_transform(x_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d99b0c-357d-4bd8-a309-6d63366ecb56",
   "metadata": {},
   "source": [
    "### 4 . What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d2347-07e2-4be0-ab3a-8213c1dcc608",
   "metadata": {},
   "source": [
    "- Feature extraction is process of reducing the dimensionality of a dataset by selecting or transforming the original features into a new set of features, which are more informative of underlying data patterns.\n",
    "- PCA is a specific technique for feature extraction that aims to transform the orginal features called principal componentsm that capture maximum variance in data. \n",
    "\n",
    "- PCA can be used for feature extraction by transforming the original feature space into new space of lower dimensionality while retaining the most important information. \n",
    "\n",
    "Steps for applying pca in feature extraction\n",
    "1. Data preprocessing\n",
    "2. Covatiance matrix calculation\n",
    "3. Elgenvalue Decomposition\n",
    "4. Selection of principal components\n",
    "5. Projection\n",
    "6. Feature Extraction \n",
    "7. Dimensionality reduction\n",
    "8. Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17cf365b-9427-4935-9d55-16fa8e5a049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1288\n",
      "Number of features per sample: 1850\n",
      "Original data shape: (1288, 1850)\n",
      "Transformed data shape after PCA: (1288, 100)\n"
     ]
    }
   ],
   "source": [
    "## importing required modules\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Load the LFW dataset containing facial images\n",
    "lfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "## Extract the images\n",
    "faces_data = lfw_dataset.data\n",
    "n_samples, n_features = faces_data.shape\n",
    "\n",
    "print(\"Number of samples:\", n_samples)\n",
    "print(\"Number of features per sample:\", n_features)\n",
    "\n",
    "## Perform PCA for feature extraction\n",
    "n_components = 100  ## number of principal components to retain\n",
    "pca = PCA(n_components=n_components)\n",
    "faces_pca = pca.fit_transform(faces_data)\n",
    "\n",
    "## printing shpae to show difference \n",
    "print(\"Original data shape:\", faces_data.shape)\n",
    "print(\"Transformed data shape after PCA:\", faces_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff517e-131b-4a2f-92e7-a2e205b7208f",
   "metadata": {},
   "source": [
    "### 5 . You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a98ee-1bdc-4b5f-a743-b684a51ff35c",
   "metadata": {},
   "source": [
    "- Min max scaling is a preprocessing techniques used to transform features to a common scale, typically within range of 0 to 1. this normalizaton helps in standardizing the range of independent varibales, making comparisions more meaningful. \n",
    "\n",
    "1. Identify features in dataset to scale. in this case \"Price,Rating.Delivery time\"\n",
    "2. Compute Min-max values for each feature in dataset. these values will be used to scale data.\n",
    "3. Apply Min-Max scaling using sklearn modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f41b4-5938-4f15-a231-dea138bddd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using min-max scaling \n",
    "## data\n",
    "## importing minmax scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## data \n",
    "data = \"Price Rating Delivery Time\"\n",
    "\n",
    "## initalizing with object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "## fit the scaler to data\n",
    "scaler.fit(data)\n",
    "\n",
    "## Transform the data using scaler\n",
    "scaled_data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740ddf0-43e1-41fe-9041-d025fa8367c9",
   "metadata": {},
   "source": [
    "### 6 . You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0550b-5803-452f-9214-f25fb67865cf",
   "metadata": {},
   "source": [
    "- Principal Component Analysis (PCA) is used for dimensionality reduction. particularly in cases where datasets contain many features.\n",
    "\n",
    "1. Understand dataset, identify all the features available including company financial data\n",
    "2. standardization or normalize the data. it is crucial for PCA to work effectively. you can use techniques like z-score or min max scaling for this step.\n",
    "3. Apply PCA to reduce dimensionality of the dataset. it works by trransforming the original features into a new set of orthogonal features called principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bd4c2-95d8-4692-a0c4-8987225c5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA to reduce dimensionality of financial data and market trends\n",
    "## import modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## data \n",
    "data = \"Financial data of company\"\n",
    "\n",
    "## initialize PCA \n",
    "pca = PCA(n_components=0.8)\n",
    "\n",
    "## fit PCA to data\n",
    "pca.fit(data)\n",
    "\n",
    "## transform data\n",
    "pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f05ae-6237-471c-8e78-3ea56aef780e",
   "metadata": {},
   "source": [
    "### 7 . For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "838ba7a5-f5d3-4a52-bfe3-2263cd048a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import modules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "## data\n",
    "data = [1,5,10,15,20]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "## initializing the Minmax sclaer\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "## performing the min max scaling on the data \n",
    "scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9b753-e996-49d3-b33a-664d531ee6d3",
   "metadata": {},
   "source": [
    "### 8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87f013-33df-42c1-ae86-8062ff4199ad",
   "metadata": {},
   "source": [
    "- To determine the number of principal components to retain in PCA for feature extraction, you typically consider explained variance ratio associated with each principal component. the explained variance ratio tells us the proportion of the dataset variance that lies along each principal component.\n",
    "\n",
    "1. Compute PCA : perform pca on dataset containing features of height,weight,age,gender,blood pressure\n",
    "2. Compute Explained variance ratio : this ratio tells the proportion of variance explained by each component. \n",
    "3. calculate Cummulative explained variance by summing up the explained variance ratio for each principal component. this cumulative explained variance explains the variance of retained principal components. \n",
    "4. select number of principal components to retain on the cumulative explain variance. typically we retain enough components to explain significant portion of the variance in dataset, around 80-90% ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
